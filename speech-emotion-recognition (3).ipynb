{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update\n!apt-get install -y libsndfile1\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\nimport IPython.display as ipd\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.metrics\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n#from sklearn.metrics import confusion_matrix, classification_reportss\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\nfrom glob import glob\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-09T00:49:41.288386Z","iopub.execute_input":"2022-12-09T00:49:41.288763Z","iopub.status.idle":"2022-12-09T00:49:46.924456Z","shell.execute_reply.started":"2022-12-09T00:49:41.288731Z","shell.execute_reply":"2022-12-09T00:49:46.923135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:52.129830Z","iopub.execute_input":"2022-12-09T00:49:52.130254Z","iopub.status.idle":"2022-12-09T00:49:52.135792Z","shell.execute_reply.started":"2022-12-09T00:49:52.130200Z","shell.execute_reply":"2022-12-09T00:49:52.134368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\nlables=[]\npath=[]","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:52.685890Z","iopub.execute_input":"2022-12-09T00:49:52.686299Z","iopub.status.idle":"2022-12-09T00:49:52.697818Z","shell.execute_reply.started":"2022-12-09T00:49:52.686262Z","shell.execute_reply":"2022-12-09T00:49:52.696694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n   \n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        lables.append(int(part[2]))\n        path.append(Ravdess + dir + '/' + file)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:53.153087Z","iopub.execute_input":"2022-12-09T00:49:53.153753Z","iopub.status.idle":"2022-12-09T00:49:53.193785Z","shell.execute_reply.started":"2022-12-09T00:49:53.153715Z","shell.execute_reply":"2022-12-09T00:49:53.192765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['Speech']=path\ndf['Lable']=lables\ndf['emotion']=None","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:53.513800Z","iopub.execute_input":"2022-12-09T00:49:53.514484Z","iopub.status.idle":"2022-12-09T00:49:53.524250Z","shell.execute_reply.started":"2022-12-09T00:49:53.514448Z","shell.execute_reply":"2022-12-09T00:49:53.523201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:53.986261Z","iopub.execute_input":"2022-12-09T00:49:53.987332Z","iopub.status.idle":"2022-12-09T00:49:53.999420Z","shell.execute_reply.started":"2022-12-09T00:49:53.987283Z","shell.execute_reply":"2022-12-09T00:49:53.998221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data visualization**","metadata":{}},{"cell_type":"markdown","source":"# **Emotion**:\n**(01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)**","metadata":{}},{"cell_type":"code","source":"switcher = {\n        1: \"neutral\",\n        2: \"calm\",\n        3: \"happy\",\n        4: \"sad\",\n        5: \"angry\",\n        6: \"fearful\",\n        7: \"disgust\",\n        8: \"surprised\",\n    }","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:55.199888Z","iopub.execute_input":"2022-12-09T00:49:55.200617Z","iopub.status.idle":"2022-12-09T00:49:55.206135Z","shell.execute_reply.started":"2022-12-09T00:49:55.200581Z","shell.execute_reply":"2022-12-09T00:49:55.204977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:55.601267Z","iopub.execute_input":"2022-12-09T00:49:55.602261Z","iopub.status.idle":"2022-12-09T00:49:55.614426Z","shell.execute_reply.started":"2022-12-09T00:49:55.602195Z","shell.execute_reply":"2022-12-09T00:49:55.613362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for j in range(1,9):\n    for i in range(df.shape[0]):\n        if(j==df['Lable'][i]):\n            df['emotion'][i]=switcher.get(j)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:55.967740Z","iopub.execute_input":"2022-12-09T00:49:55.968475Z","iopub.status.idle":"2022-12-09T00:49:56.507261Z","shell.execute_reply.started":"2022-12-09T00:49:55.968437Z","shell.execute_reply":"2022-12-09T00:49:56.506242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:56.509005Z","iopub.execute_input":"2022-12-09T00:49:56.509382Z","iopub.status.idle":"2022-12-09T00:49:56.519757Z","shell.execute_reply.started":"2022-12-09T00:49:56.509348Z","shell.execute_reply":"2022-12-09T00:49:56.518710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nipd.Audio(df['Speech'][2])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:56.948585Z","iopub.execute_input":"2022-12-09T00:49:56.949296Z","iopub.status.idle":"2022-12-09T00:49:56.967832Z","shell.execute_reply.started":"2022-12-09T00:49:56.949235Z","shell.execute_reply":"2022-12-09T00:49:56.966924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import cycle\n\nsns.set_theme(style=\"white\", palette=None)\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:57.469152Z","iopub.execute_input":"2022-12-09T00:49:57.470531Z","iopub.status.idle":"2022-12-09T00:49:57.477265Z","shell.execute_reply.started":"2022-12-09T00:49:57.470483Z","shell.execute_reply":"2022-12-09T00:49:57.476284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Getting the sample rate of audio file using librosa**","metadata":{}},{"cell_type":"code","source":"y, sr = librosa.load(df['Speech'][2])\n\nprint(f'sr: {sr}')","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:58.305809Z","iopub.execute_input":"2022-12-09T00:49:58.306516Z","iopub.status.idle":"2022-12-09T00:49:58.468315Z","shell.execute_reply.started":"2022-12-09T00:49:58.306481Z","shell.execute_reply":"2022-12-09T00:49:58.467292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Plotting signal**","metadata":{}},{"cell_type":"code","source":"pd.Series(y).plot(figsize=(10, 5),\n                  lw=1,\n                  title=df['emotion'][2]+\" \"+'Raw Audio Example',\n                  )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:49:59.225714Z","iopub.execute_input":"2022-12-09T00:49:59.226085Z","iopub.status.idle":"2022-12-09T00:49:59.482100Z","shell.execute_reply.started":"2022-12-09T00:49:59.226053Z","shell.execute_reply":"2022-12-09T00:49:59.481143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **signal after trimming silence**","metadata":{}},{"cell_type":"code","source":"y_trimmed, _ = librosa.effects.trim(y, top_db=20)\npd.Series(y_trimmed).plot(figsize=(10, 5),\n                  lw=1,\n                  title=df['emotion'][2]+\" \"+'Raw Audio Trimmed Example',\n                 color=color_pal[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:50:00.198288Z","iopub.execute_input":"2022-12-09T00:50:00.198981Z","iopub.status.idle":"2022-12-09T00:50:00.450259Z","shell.execute_reply.started":"2022-12-09T00:50:00.198942Z","shell.execute_reply":"2022-12-09T00:50:00.449175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Spectogram**","metadata":{}},{"cell_type":"code","source":"D = librosa.stft(y)\nS_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\nS_db.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:50:00.966646Z","iopub.execute_input":"2022-12-09T00:50:00.967028Z","iopub.status.idle":"2022-12-09T00:50:00.988772Z","shell.execute_reply.started":"2022-12-09T00:50:00.966997Z","shell.execute_reply":"2022-12-09T00:50:00.987821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the transformed audio data\nfig, ax = plt.subplots(figsize=(10, 5))\nimg = librosa.display.specshow(S_db,\n                              x_axis='time',\n                              y_axis='log',\n                              ax=ax)\nax.set_title('Spectogram Example', fontsize=20)\nfig.colorbar(img, ax=ax, format=f'%0.2f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:50:01.476617Z","iopub.execute_input":"2022-12-09T00:50:01.476986Z","iopub.status.idle":"2022-12-09T00:50:01.935101Z","shell.execute_reply.started":"2022-12-09T00:50:01.476953Z","shell.execute_reply":"2022-12-09T00:50:01.934167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature extraction**","metadata":{}},{"cell_type":"code","source":"def extract_MFCC(file):\n    \n    y, sr = librosa.load(file,duration=3,offset=0.5)\n    mfcc=np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40).T,axis=0)\n    return mfcc","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:50:02.458680Z","iopub.execute_input":"2022-12-09T00:50:02.459457Z","iopub.status.idle":"2022-12-09T00:50:02.465911Z","shell.execute_reply.started":"2022-12-09T00:50:02.459408Z","shell.execute_reply":"2022-12-09T00:50:02.464742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_mfcc=df['Speech'].apply(lambda x:extract_MFCC(x))","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:50:02.938556Z","iopub.execute_input":"2022-12-09T00:50:02.938931Z","iopub.status.idle":"2022-12-09T00:54:47.017243Z","shell.execute_reply.started":"2022-12-09T00:50:02.938898Z","shell.execute_reply":"2022-12-09T00:54:47.015584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=[x for x in X_mfcc]\nX=np.array(X)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-09T00:54:47.019582Z","iopub.execute_input":"2022-12-09T00:54:47.019938Z","iopub.status.idle":"2022-12-09T00:54:47.031194Z","shell.execute_reply.started":"2022-12-09T00:54:47.019903Z","shell.execute_reply":"2022-12-09T00:54:47.030199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\ny = enc.fit_transform(df[['Lable']])\ny = y.toarray()\n# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True)\n# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train=np.expand_dims(x_train,-1)\nx_test=np.expand_dims(x_test,-1)\n\n\n\n\n\n     \n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-09T01:06:50.722091Z","iopub.execute_input":"2022-12-09T01:06:50.722818Z","iopub.status.idle":"2022-12-09T01:06:50.736259Z","shell.execute_reply.started":"2022-12-09T01:06:50.722780Z","shell.execute_reply":"2022-12-09T01:06:50.735124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\n\nmodel=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=8, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-09T01:06:53.110526Z","iopub.execute_input":"2022-12-09T01:06:53.112861Z","iopub.status.idle":"2022-12-09T01:06:53.207400Z","shell.execute_reply.started":"2022-12-09T01:06:53.112825Z","shell.execute_reply":"2022-12-09T01:06:53.205649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2022-12-09T01:06:54.355188Z","iopub.execute_input":"2022-12-09T01:06:54.357783Z","iopub.status.idle":"2022-12-09T01:07:06.996582Z","shell.execute_reply.started":"2022-12-09T01:06:54.357744Z","shell.execute_reply":"2022-12-09T01:07:06.995610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-12-09T01:07:06.998648Z","iopub.execute_input":"2022-12-09T01:07:06.999682Z","iopub.status.idle":"2022-12-09T01:07:07.109643Z","shell.execute_reply.started":"2022-12-09T01:07:06.999643Z","shell.execute_reply":"2022-12-09T01:07:07.108540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}